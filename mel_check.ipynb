{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# melのチェックを行う用のノートブック"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ここに確認したいwavを配置.\r\n",
    "\r\n",
    "input_wav_paths = ['./pre_voice/Universal/jsut_ver1.1_BASIC5000_0001.wav', './pre_voice/Universal/VCTK-Corpus_p257_256.wav']\r\n",
    "\r\n",
    "p_config = './config/JSUT_JSSS/preprocess.yaml'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# まずは, NARS2Sのほう.\r\n",
    "import yaml\r\n",
    "\r\n",
    "from utils.utils import get_mels, plot_mels\r\n",
    "\r\n",
    "preprocess_config = yaml.load(\r\n",
    "        open(p_config, \"r\", encoding='utf-8'), Loader=yaml.FullLoader\r\n",
    "    )\r\n",
    "mels = get_mels(wav_paths=input_wav_paths, config=preprocess_config, mel_num=80)\r\n",
    "\r\n",
    "plot_mels(mels, input_wav_paths, sr=preprocess_config[\"preprocessing\"][\"audio\"][\"sampling_rate\"], sharex=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# hifiganのものでmelを作ってみる.\r\n",
    "import os\r\n",
    "\r\n",
    "from tqdm import tqdm\r\n",
    "import torch\r\n",
    "\r\n",
    "from hifigan.meldataset import load_wav, mel_spectrogram\r\n",
    "\r\n",
    "mels = []\r\n",
    "\r\n",
    "sr = preprocess_config[\"preprocessing\"][\"audio\"][\"sampling_rate\"]\r\n",
    "n_fft = preprocess_config[\"preprocessing\"][\"stft\"][\"filter_length\"]\r\n",
    "num_mels = preprocess_config[\"preprocessing\"][\"mel\"][\"n_mel_channels\"]\r\n",
    "hop_size = preprocess_config[\"preprocessing\"][\"stft\"][\"hop_length\"]\r\n",
    "win_size = preprocess_config[\"preprocessing\"][\"stft\"][\"win_length\"]\r\n",
    "fmin = preprocess_config[\"preprocessing\"][\"mel\"][\"mel_fmin\"]\r\n",
    "fmax = preprocess_config[\"preprocessing\"][\"mel\"][\"mel_fmax\"]\r\n",
    "\r\n",
    "for wav_path in tqdm(input_wav_paths):\r\n",
    "    audio, sampling_rate = load_wav(wav_path, sr)\r\n",
    "    assert sampling_rate == sr\r\n",
    "    # audio = audio / 32768.0\r\n",
    "    audio = torch.FloatTensor(audio).to(\"cuda\")\r\n",
    "    audio = audio.unsqueeze(0)\r\n",
    "\r\n",
    "    mel = mel_spectrogram(audio, n_fft, num_mels, sr,\r\n",
    "                            hop_size, win_size, fmin, fmax)\r\n",
    "    mel = mel.squeeze(0).cpu().numpy()\r\n",
    "    mels.append(mel)\r\n",
    "\r\n",
    "plot_mels(mels, input_wav_paths, sr=sr, sharex=False)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import shutil\r\n",
    "import os\r\n",
    "from glob import glob\r\n",
    "from tqdm import tqdm\r\n",
    "\r\n",
    "input_path = './raw_data/JSUT_JSSS/JSSS'\r\n",
    "output_path = './raw_data/JSUT_to_from_JSSS/target'\r\n",
    "\r\n",
    "os.makedirs(output_path, exist_ok=True)\r\n",
    "for t_path in tqdm(glob(os.path.join(input_path,'*.wav'))):\r\n",
    "    output_wav_path = os.path.join(output_path, \"JSUT2JSSS_\"+os.path.basename(t_path))\r\n",
    "    shutil.copy(t_path, output_wav_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import shutil\r\n",
    "import os\r\n",
    "from glob import glob\r\n",
    "from tqdm import tqdm\r\n",
    "\r\n",
    "input_path = './raw_data/JSUT_to_from_JSSS'\r\n",
    "\r\n",
    "for t_path in tqdm(glob(os.path.join(input_path,'**/*.wav'), recursive=True)):\r\n",
    "    output_wav_path = t_path\r\n",
    "    if \"JSSS2JSUT\" in t_path:\r\n",
    "        output_wav_path = output_wav_path.replace(\"JSSS2JSUT\", \"JSSS_JSUT\")\r\n",
    "    elif \"JSUT2JSSS\" in t_path:\r\n",
    "        output_wav_path = output_wav_path.replace(\"JSUT2JSSS\", \"JSUT_JSSS\")\r\n",
    "    os.rename(t_path, output_wav_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ちゃんと存在しているかチェック(さっきはしていなかった)\r\n",
    "import glob\r\n",
    "import os\r\n",
    "import tqdm\r\n",
    "\r\n",
    "s_wav_path = glob.glob('./pre_voice/jsut_jsss_jvs/source/*.wav')\r\n",
    "t_wav_path = glob.glob('./pre_voice/jsut_jsss_jvs/target/*.wav')\r\n",
    "\r\n",
    "for a, b in tqdm.tqdm(zip(s_wav_path, t_wav_path)):\r\n",
    "    assert os.path.basename(a) == os.path.basename(b), print(a,b)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ちゃんと存在しているかチェック(さっきはしていなかった)\r\n",
    "import glob\r\n",
    "import os\r\n",
    "import tqdm\r\n",
    "\r\n",
    "s_wav_path = glob.glob('./preprocessed_data/jsut_jsss_jvs/source/mel/*.npy')\r\n",
    "t_wav_path = glob.glob('./preprocessed_data/jsut_jsss_jvs/target/mel/*.npy')\r\n",
    "\r\n",
    "for a, b in tqdm.tqdm(zip(s_wav_path, t_wav_path)):\r\n",
    "    assert os.path.basename(a) == os.path.basename(b), print(a,b)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from glob import glob\r\n",
    "import os.path as opth\r\n",
    "\r\n",
    "mel_paths_s = glob('./preprocessed_data/jsut_jsss_jvs/source/mel/*.npy')\r\n",
    "mel_paths_t = glob('./preprocessed_data/jsut_jsss_jvs/target/mel/*.npy')\r\n",
    "\r\n",
    "s_mel_base = set([opth.basename(path_) for path_ in mel_paths_s])\r\n",
    "t_mel_base = set([opth.basename(path_) for path_ in mel_paths_t])\r\n",
    "\r\n",
    "print(len(s_mel_base))\r\n",
    "print(len(t_mel_base))\r\n",
    "print(len(s_mel_base&t_mel_base))\r\n",
    "\r\n",
    "s_mel_diff = s_mel_base - t_mel_base # surceから消したいもの\r\n",
    "t_mel_diff = t_mel_base - s_mel_base # targetから消したいもの\r\n",
    "\r\n",
    "for s_mel_path in s_mel_diff:\r\n",
    "    os.remove('./preprocessed_data/jsut_jsss_jvs/source/mel/' + s_mel_path)\r\n",
    "    base_name = s_mel_path.replace(\"mel-\", \"\")\r\n",
    "\r\n",
    "    os.remove('./preprocessed_data/jsut_jsss_jvs/source/energy/' + \"energy-\"+base_name)\r\n",
    "    os.remove('./preprocessed_data/jsut_jsss_jvs/source/pitch/' + \"pitch-\"+base_name)\r\n",
    "\r\n",
    "for t_mel_path in t_mel_diff:\r\n",
    "    os.remove('./preprocessed_data/jsut_jsss_jvs/target/mel/' + t_mel_path)\r\n",
    "    base_name = t_mel_path.replace(\"mel-\", \"\")\r\n",
    "\r\n",
    "    os.remove('./preprocessed_data/jsut_jsss_jvs/target/energy/' + \"energy-\"+base_name)\r\n",
    "    os.remove('./preprocessed_data/jsut_jsss_jvs/target/pitch/' + \"pitch-\"+base_name)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ちゃんと存在しているかチェック(さっきはしていなかった)\r\n",
    "import glob\r\n",
    "import os\r\n",
    "import tqdm\r\n",
    "s_wav_path = glob.glob('./preprocessed_data/jsut_jsss_jvs/source/mel/*.npy')\r\n",
    "t_wav_path = glob.glob('./preprocessed_data/jsut_jsss_jvs/target/mel/*.npy')\r\n",
    "\r\n",
    "for a, b in tqdm.tqdm(zip(s_wav_path, t_wav_path)):\r\n",
    "    assert os.path.basename(a) == os.path.basename(b), print(a,b)\r\n",
    "\r\n",
    "s_wav_path = glob.glob('./preprocessed_data/jsut_jsss_jvs/source/energy/*.npy')\r\n",
    "t_wav_path = glob.glob('./preprocessed_data/jsut_jsss_jvs/target/energy/*.npy')\r\n",
    "\r\n",
    "for a, b in tqdm.tqdm(zip(s_wav_path, t_wav_path)):\r\n",
    "    assert os.path.basename(a) == os.path.basename(b), print(a,b)\r\n",
    "\r\n",
    "s_wav_path = glob.glob('./preprocessed_data/jsut_jsss_jvs/source/pitch/*.npy')\r\n",
    "t_wav_path = glob.glob('./preprocessed_data/jsut_jsss_jvs/target/pitch/*.npy')\r\n",
    "\r\n",
    "for a, b in tqdm.tqdm(zip(s_wav_path, t_wav_path)):\r\n",
    "    assert os.path.basename(a) == os.path.basename(b), print(a,b)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np \r\n",
    "from glob import glob\r\n",
    "\r\n",
    "val_size = 1000\r\n",
    "out_dir = \"./preprocessed_data/jsut_jsss_jvs\"\r\n",
    "\r\n",
    "mel_paths_s = glob('./preprocessed_data/jsut_jsss_jvs/source/mel/*.npy')\r\n",
    "\r\n",
    "out = []\r\n",
    "speakers = []\r\n",
    "for s_mel_path in mel_paths_s:\r\n",
    "    s_mel_path = os.path.basename(s_mel_path).replace(\".npy\", \"\")\r\n",
    "    base_name = s_mel_path.replace(\"mel-\", \"\")\r\n",
    "    out.append(base_name)\r\n",
    "    speakers.append(base_name.split('_')[0])\r\n",
    "    speakers.append(base_name.split('_')[1])\r\n",
    "\r\n",
    "out = np.array(out)\r\n",
    "index_ = np.random.permutation(len(out))\r\n",
    "train_outs = out[index_[val_size:]]\r\n",
    "valid_outs = out[index_[: val_size]]\r\n",
    "\r\n",
    "for i, source_or_target in enumerate([\"source\", \"target\"]):\r\n",
    "    # Write metadata\r\n",
    "    with open(os.path.join(out_dir, source_or_target, \"train.txt\"), \"w\", encoding=\"utf-8\") as f:\r\n",
    "        for m in train_outs:\r\n",
    "            f.write(m + \"\\n\")\r\n",
    "    with open(os.path.join(out_dir, source_or_target, \"val.txt\"), \"w\", encoding=\"utf-8\") as f:\r\n",
    "        for m in valid_outs:\r\n",
    "            f.write(m + \"\\n\")\r\n",
    "\r\n",
    "assert train_outs[0][0] == train_outs[1][0]\r\n",
    "assert valid_outs[0][0] == valid_outs[1][0]\r\n",
    "\r\n",
    "speakers = set(speakers)\r\n",
    "with open(os.path.join(out_dir, \"speakers.txt\"), \"w\", encoding=\"utf-8\") as f:\r\n",
    "    for speaker in speakers:\r\n",
    "        f.write(speaker + '\\n')\r\n",
    "print(\"正しく想定したspeakersが記録されたか確認してください.\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ないとは思うが, pitch, energy, mel, duration全てのshapeが一致しているかチェック.\r\n",
    "# jvs006_jvs016_048.npyあかん\r\n",
    "# jvs031_jvs016_048\r\n",
    "# jvs055_jvs016_048\r\n",
    "# jvs081_jvs016_048\r\n",
    "\r\n",
    "from glob import glob \r\n",
    "import os \r\n",
    "import numpy as np\r\n",
    "from tqdm import tqdm\r\n",
    "\r\n",
    "duration_path = glob(\"./preprocessed_data/jsut_jsss_jvs_2/source/duration/*.npy\")\r\n",
    "reduction_factor = 3\r\n",
    "\r\n",
    "# for i, path_ in tqdm(enumerate(duration_path)):\r\n",
    "#     if i < 0:\r\n",
    "#         continue\r\n",
    "#     basename = os.path.basename(path_).replace(\"duration-\", \"\")\r\n",
    "#     if basename in [\"jvs006_jvs016_048.npy\", \"jvs009_jvs002_095.npy\", \"jvs009_jvs058_095.npy\", \"jvs009_jvs064_095.npy\", \"jvs009_jvs098_095.npy\", \"jvs017_jvs009_095.npy\", \"jvs031_jvs016_048.npy\", \"jvs043_jvs009_095.npy\", \"jvs055_jvs016_048.npy\", \"jvs067_jvs009_095.npy\", \"jvs081_jvs016_048.npy\", \"jvs093_jvs009_095.npy\"]:\r\n",
    "#         continue\r\n",
    "#     duration = np.load(path_)\r\n",
    "#     mel = np.load(os.path.join(\"./preprocessed_data/jsut_jsss_jvs_2/target/mel/\", \"mel-\"+basename))\r\n",
    "#     energy = np.load(os.path.join(\"./preprocessed_data/jsut_jsss_jvs_2/target/energy/\", \"energy-\"+basename))\r\n",
    "#     pitch = np.load(os.path.join(\"./preprocessed_data/jsut_jsss_jvs_2/target/pitch/\", \"pitch-\"+basename))\r\n",
    "#     assert duration.sum() == mel.shape[0]\r\n",
    "#     assert duration.sum() == energy.shape[0]\r\n",
    "#     assert duration.sum() == pitch.shape[0], print(basename)\r\n",
    "\r\n",
    "\r\n",
    "for i, path_ in tqdm(enumerate(duration_path)):\r\n",
    "    if i < 0:\r\n",
    "        continue\r\n",
    "    basename = os.path.basename(path_).replace(\"duration-\", \"\")\r\n",
    "    if basename in [\"jvs006_jvs016_048.npy\", \"jvs009_jvs002_095.npy\", \"jvs009_jvs058_095.npy\", \"jvs009_jvs064_095.npy\", \"jvs009_jvs098_095.npy\", \"jvs017_jvs009_095.npy\", \"jvs031_jvs016_048.npy\", \"jvs043_jvs009_095.npy\", \"jvs055_jvs016_048.npy\", \"jvs067_jvs009_095.npy\", \"jvs081_jvs016_048.npy\", \"jvs093_jvs009_095.npy\", \"jvs016_jvs024_048.npy\", \"jvs016_jvs027_048.npy\", \"jvs016_jvs036_048.npy\", \"jvs016_jvs044_048.npy\"]:\r\n",
    "        continue\r\n",
    "    duration = np.load(path_)\r\n",
    "    mel = np.load(os.path.join(\"./preprocessed_data/jsut_jsss_jvs_2/source/mel/\", \"mel-\"+basename))\r\n",
    "    energy = np.load(os.path.join(\"./preprocessed_data/jsut_jsss_jvs_2/source/energy/\", \"energy-\"+basename))\r\n",
    "    pitch = np.load(os.path.join(\"./preprocessed_data/jsut_jsss_jvs_2/source/pitch/\", \"pitch-\"+basename))\r\n",
    "    \r\n",
    "    # print(duration.shape, (mel.shape[0]+2)//reduction_factor, (energy.shape[0]+2)//reduction_factor)\r\n",
    "    assert duration.shape[0] == (mel.shape[0]+2)//reduction_factor, print(basename, duration.shape, mel.shape)\r\n",
    "    assert mel.shape[0] == energy.shape[0], print(basename)\r\n",
    "    assert mel.shape[0] == pitch.shape[0], print(basename)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import librosa\r\n",
    "import numpy as np\r\n",
    "from scipy.io.wavfile import read\r\n",
    "\r\n",
    "# def load_wav(full_path, sr):\r\n",
    "#     # sampling_rate, data = read(full_path)\r\n",
    "#     wav, _ = librosa.load(\r\n",
    "#         full_path, sr=sr)\r\n",
    "#     return wav, sr\r\n",
    "\r\n",
    "def load_wav(full_path):\r\n",
    "    sampling_rate, data = read(full_path)\r\n",
    "    return data, sampling_rate\r\n",
    "\r\n",
    "\r\n",
    "wav, _ = load_wav(\"./raw_data/LJSpeech/LJSpeech-1.1_LJ050-0264.wav\")\r\n",
    "# wav, _ = load_wav(\"./pre_voice/Universal/jsut_ver1.1_BASIC5000_0001.wav\", sr=24000)\r\n",
    "# wav, _ = load_wav(\"./pre_voice/jsut_jsss_jvs/source/JSSS_JSUT_BASIC5000_0001.wav\", sr=22050)\r\n",
    "\r\n",
    "print(wav.dtype)\r\n",
    "print(np.max(wav))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from glob import glob \r\n",
    "import os \r\n",
    "import numpy as np\r\n",
    "import shutil\r\n",
    "from tqdm import tqdm\r\n",
    "\r\n",
    "output_path = \"./output/mel_for_hifi-gan/jsut_jsss_jvs_2/mels\"\r\n",
    "\r\n",
    "for mel_path in tqdm(glob(\"./preprocessed_data/jsut_jsss_jvs_2/target/mel/*.npy\")):\r\n",
    "    base_name = os.path.basename(mel_path).replace(\"mel-\", \"\")\r\n",
    "    mel = np.load(mel_path)\r\n",
    "    np.save(os.path.join(output_path, base_name), mel.T)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from utils.utils import plot_mels, get_mels\r\n",
    "import yaml\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "mel = np.load(\"./output/mel_for_hifi-gan/jsut_jsss_jvs/mels/JSSS_JSUT_BASIC5000_0002.npy\")\r\n",
    "\r\n",
    "preprocess_config = yaml.load(\r\n",
    "        open(\"./config/jsut_jsss_jvs/preprocess.yaml\", \"r\", encoding='utf-8'), Loader=yaml.FullLoader\r\n",
    "    )\r\n",
    "\r\n",
    "mel_before = get_mels([\"./pre_voice/jsut_jsss_jvs/target/JSSS_JSUT_BASIC5000_0002.wav\"], 80, preprocess_config)\r\n",
    "\r\n",
    "\r\n",
    "plot_mels([mel, *mel_before], [\"./pre_voice/jsut_jsss_jvs/target/JSSS_JSUT_BASIC5000_0002.wav\"]*2, 22050)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\r\n",
    "import os \r\n",
    "\r\n",
    "import numpy as np \r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "from utils.tools import plot_mel\r\n",
    "\r\n",
    "with open(\r\n",
    "    \"./preprocessed_data/jsut_jsss_jvs/target/stats.json\"\r\n",
    ") as f:\r\n",
    "    stats = json.load(f)\r\n",
    "    stats = stats[\"pitch\"] + stats[\"energy\"][:2]\r\n",
    "\r\n",
    "file_name = \"jvs037_jvs049_045\"\r\n",
    "file_base = \"./preprocessed_data/jsut_jsss_jvs/target\"\r\n",
    "\r\n",
    "mel = np.load(os.path.join(file_base, \"mel\", \"mel-\"+file_name+\".npy\"))\r\n",
    "pitch = np.load(os.path.join(file_base, \"pitch\", \"pitch-\"+file_name+\".npy\"))\r\n",
    "energy = np.load(os.path.join(file_base, \"energy\", \"energy-\"+file_name+\".npy\"))\r\n",
    "\r\n",
    "# pltとして, figを用意.\r\n",
    "fig = plot_mel(\r\n",
    "    [\r\n",
    "        (mel.T, pitch, energy),\r\n",
    "    ],\r\n",
    "    stats,\r\n",
    "    [\"source Spectrogram\"],\r\n",
    ")\r\n",
    "plt.savefig(f\"{file_name}.png\")\r\n",
    "plt.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# continuous pitch 実装\r\n",
    "import pyworld as pw\r\n",
    "import numpy as np\r\n",
    "import librosa\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "wav_path = \"./pre_voice/N2C/target/n_c_beyond_tokkun-08.wav\"\r\n",
    "sampling_rate = 22050\r\n",
    "hop_length = 256\r\n",
    "\r\n",
    "wav, _ = librosa.load(wav_path, sr=sampling_rate)\r\n",
    "\r\n",
    "# Compute fundamental frequency\r\n",
    "pitch, t = pw.dio(\r\n",
    "    wav.astype(np.float64),\r\n",
    "    sampling_rate,\r\n",
    "    frame_period=hop_length / sampling_rate * 1000,\r\n",
    ")\r\n",
    "pitch = pw.stonemask(wav.astype(np.float64), pitch, t, sampling_rate)\r\n",
    "\r\n",
    "plt.plot(pitch)\r\n",
    "\r\n",
    "pitch = np.where(pitch < 1e-6, np.nan, pitch)\r\n",
    "\r\n",
    "import pandas as pd \r\n",
    "\r\n",
    "df = pd.Series(pitch)\r\n",
    "df = df.interpolate()\r\n",
    "\r\n",
    "first_value = pitch[df.isnull().values.tolist().index(False)]\r\n",
    "df = df.fillna(first_value)\r\n",
    "\r\n",
    "pitch = df.values\r\n",
    "plt.plot(pitch)\r\n",
    "plt.show()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\r\n",
    "import os \r\n",
    "\r\n",
    "import numpy as np \r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "from utils.tools import plot_mel\r\n",
    "\r\n",
    "with open(\r\n",
    "    \"./preprocessed_data/N2C_3/target/stats.json\"\r\n",
    ") as f:\r\n",
    "    stats = json.load(f)\r\n",
    "    stats = stats[\"energy\"]\r\n",
    "\r\n",
    "file_name = \"n_c_beyond_tokkun-08\"\r\n",
    "file_base = \"./preprocessed_data/N2C_3/target\"\r\n",
    "\r\n",
    "mel = np.load(os.path.join(file_base, \"mel\", \"mel-\"+file_name+\".npy\"))\r\n",
    "pitch = np.load(os.path.join(file_base, \"pitch\", \"pitch-\"+file_name+\".npy\"))\r\n",
    "energy = np.load(os.path.join(file_base, \"energy\", \"energy-\"+file_name+\".npy\"))\r\n",
    "\r\n",
    "\r\n",
    "energy_min, energy_max, energy_mean, energy_std = stats\r\n",
    "\r\n",
    "energy = energy * energy_std + energy_mean\r\n",
    "\r\n",
    "plt.plot(energy)\r\n",
    "plt.show()\r\n",
    "\r\n",
    "energy = np.where(energy < -4)\r\n",
    "\r\n",
    "pitch[energy] = np.min(pitch)\r\n",
    "\r\n",
    "plt.plot(pitch)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from preprocessing.n2c_voiceprocess import devide_voice_from_path, delete_novoice_from_path\r\n",
    "from utils.utils import plot_mels\r\n",
    "import yaml \r\n",
    "\r\n",
    "input_wav_path = \"./pre_voice/JSUT_JSSS/JSSS/BASIC5000_2296.wav\"\r\n",
    "\r\n",
    "preprocess_config = yaml.load(\r\n",
    "        open(\"./config/jsut_jsss_jvs/preprocess.yaml\", \"r\", encoding='utf-8'), Loader=yaml.FullLoader\r\n",
    "    )\r\n",
    "\r\n",
    "devide_voice_from_path(input_wav_path, \"./\", preprocess_config)\r\n",
    "\r\n",
    "output_wav_path = [\"./BASIC5000_2296_0.wav\", \"./BASIC5000_2296_1.wav\"]\r\n",
    "for path_ in output_wav_path:\r\n",
    "    delete_novoice_from_path(path_, \"./\", preprocess_config)\r\n",
    "\r\n",
    "plot_mels(output_wav_path, config=preprocess_config)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import librosa\r\n",
    "import yaml\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np \r\n",
    "\r\n",
    "import audio as Audio\r\n",
    "from preprocessing.calc_duration import reduction, calc_duration\r\n",
    "from utils.utils import plot_mels\r\n",
    "\r\n",
    "source_path = \"./pre_voice/N2C/source/n_c_beyond_26.wav\"\r\n",
    "target_path = \"./pre_voice/N2C/target/n_c_beyond_26.wav\"\r\n",
    "\r\n",
    "p_config = yaml.load(\r\n",
    "        open(\"./config/jsut_jsss_jvs/preprocess.yaml\", \"r\", encoding='utf-8'), Loader=yaml.FullLoader\r\n",
    "    )\r\n",
    "\r\n",
    "STFT = Audio.stft.TacotronSTFT(\r\n",
    "        p_config[\"preprocessing\"][\"stft\"][\"filter_length\"],\r\n",
    "        p_config[\"preprocessing\"][\"stft\"][\"hop_length\"],\r\n",
    "        p_config[\"preprocessing\"][\"stft\"][\"win_length\"],\r\n",
    "        20,  # config[\"preprocessing\"][\"mel\"][\"n_mel_channels\"]のところ.\r\n",
    "        p_config[\"preprocessing\"][\"audio\"][\"sampling_rate\"],\r\n",
    "        p_config[\"preprocessing\"][\"mel\"][\"mel_fmin\"],\r\n",
    "        p_config[\"preprocessing\"][\"mel\"][\"mel_fmax\"],\r\n",
    "    )\r\n",
    "reduction_factor = 3\r\n",
    "source_wav, _ = librosa.load(\r\n",
    "    source_path, sr=p_config[\"preprocessing\"][\"audio\"][\"sampling_rate\"])\r\n",
    "target_wav, _ = librosa.load(\r\n",
    "    target_path, sr=p_config[\"preprocessing\"][\"audio\"][\"sampling_rate\"])\r\n",
    "source_mel, energy = Audio.tools.get_mel_from_wav(source_wav, STFT)\r\n",
    "target_mel, _ = Audio.tools.get_mel_from_wav(target_wav, STFT)\r\n",
    "\r\n",
    "source_mel = reduction(source_mel, reduction_factor)\r\n",
    "energy = reduction(energy, reduction_factor)\r\n",
    "print(10**(-40/20))\r\n",
    "print(np.log(10**(-40/20)))\r\n",
    "\r\n",
    "print(np.log(energy+1e-6))\r\n",
    "\r\n",
    "plt.plot(np.log(energy+1e-6))\r\n",
    "target_mel = reduction(target_mel, reduction_factor)\r\n",
    "\r\n",
    "plot_mels(wav_paths=[source_path, target_path], mels=[source_mel, target_mel])\r\n",
    "\r\n",
    "duration = calc_duration([target_mel, source_mel], target_path, np.log(energy+1e-6) < -4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_wav_path = \"./BASIC5000_2296.wav\"\r\n",
    "# input_wav_path = \"./pre_voice/JSUT_JSSS/JSSS/BASIC5000_2296.wav\"\r\n",
    "\r\n",
    "p_config = yaml.load(\r\n",
    "        open(\"./config/jsut_jsss_jvs/preprocess.yaml\", \"r\", encoding='utf-8'), Loader=yaml.FullLoader\r\n",
    "    )\r\n",
    "\r\n",
    "STFT = Audio.stft.TacotronSTFT(\r\n",
    "        p_config[\"preprocessing\"][\"stft\"][\"filter_length\"],\r\n",
    "        p_config[\"preprocessing\"][\"stft\"][\"hop_length\"],\r\n",
    "        p_config[\"preprocessing\"][\"stft\"][\"win_length\"],\r\n",
    "        20,  # config[\"preprocessing\"][\"mel\"][\"n_mel_channels\"]のところ.\r\n",
    "        p_config[\"preprocessing\"][\"audio\"][\"sampling_rate\"],\r\n",
    "        p_config[\"preprocessing\"][\"mel\"][\"mel_fmin\"],\r\n",
    "        p_config[\"preprocessing\"][\"mel\"][\"mel_fmax\"],\r\n",
    "    )\r\n",
    "reduction_factor = 3\r\n",
    "source_wav, _ = librosa.load(\r\n",
    "    input_wav_path, sr=p_config[\"preprocessing\"][\"audio\"][\"sampling_rate\"])\r\n",
    "source_mel, energy = Audio.tools.get_mel_from_wav(source_wav, STFT)\r\n",
    "source_mel = reduction(source_mel, reduction_factor)\r\n",
    "energy = reduction(energy, reduction_factor)\r\n",
    "\r\n",
    "print(np.log(energy+1e-6))\r\n",
    "plt.plot(np.log(energy+1e-6))\r\n",
    "plot_mels(wav_paths=[input_wav_path], config=p_config)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pydub import AudioSegment, silence\r\n",
    "import os.path as opth\r\n",
    "\r\n",
    "input_wav_path = \"./pre_voice/JSUT_JSSS/JSSS/BASIC5000_2296.wav\"\r\n",
    "\r\n",
    "def delete_novoice_from_path(input_path, output_path, preprocess_config, chunk_size=50):\r\n",
    "    \"\"\"無音区間を先頭と末尾から削除します.\r\n",
    "    Args:\r\n",
    "      input_path: wavファイルへのpath\r\n",
    "      output_path: wavファイルを貯めたい場所. ファイル名はinput_pathのbasenameからとる.\r\n",
    "      chunk_size: 削除に用いる音声の最小単位. 基本defaultのままで良さそう.\r\n",
    "    \"\"\"\r\n",
    "    silence_thresh = preprocess_config[\"preprocessing\"][\"audio\"][\"silence_thresh\"]\r\n",
    "    silence_thresh_h = preprocess_config[\"preprocessing\"][\"audio\"][\"silence_thresh_head\"]\r\n",
    "\r\n",
    "    if (silence_thresh_h is not None) and (silence_thresh_h < silence_thresh):\r\n",
    "        print(\"Warning: 基本的に, silene_thresh_hの方が, silence_threshよりも大きいべきです.\")\r\n",
    "\r\n",
    "    audio = AudioSegment.from_wav(input_path)\r\n",
    "\r\n",
    "    # 参考: https://stackoverflow.com/questions/29547218/\r\n",
    "    # remove-silence-at-the-beginning-and-at-the-end-of-wave-files-with-pydub\r\n",
    "    def detect_leading_silence(sound, silence_threshold=-50.0, chunk_size=10):\r\n",
    "        trim_ms = 0  # ms\r\n",
    "\r\n",
    "        assert chunk_size > 0  # to avoid infinite loop\r\n",
    "        while sound[trim_ms:trim_ms+chunk_size].dBFS < silence_threshold and trim_ms < len(sound):\r\n",
    "            trim_ms += chunk_size\r\n",
    "\r\n",
    "        return trim_ms\r\n",
    "\r\n",
    "    if silence_thresh_h is None:\r\n",
    "        silence_thresh_h = silence_thresh\r\n",
    "\r\n",
    "    start_trim = detect_leading_silence(audio, silence_threshold=silence_thresh_h, chunk_size=chunk_size)\r\n",
    "    end_trim = detect_leading_silence(audio.reverse(),\r\n",
    "                                      silence_threshold=silence_thresh, chunk_size=chunk_size)\r\n",
    "\r\n",
    "    duration = len(audio)\r\n",
    "    audio_cut = audio[start_trim:duration-end_trim]\r\n",
    "    silences = silence.detect_silence(audio_cut, min_silence_len=50, silence_thresh=silence_thresh)\r\n",
    "\r\n",
    "    audio_cut_new = AudioSegment.empty()\r\n",
    "    s_index = 0\r\n",
    "    for silence_ in silences:\r\n",
    "        audio_cut_new += audio_cut[s_index:silence_[0]]\r\n",
    "        audio_cut_new += AudioSegment.silent(duration=silence_[1]-silence_[0])\r\n",
    "        s_index = silence_[1]\r\n",
    "\r\n",
    "    audio_cut_new += audio_cut[s_index:]\r\n",
    "    audio_cut_new.export(opth.join(output_path, opth.basename(input_path)), format=\"wav\")\r\n",
    "\r\n",
    "delete_novoice_from_path(input_wav_path, './', p_config)\r\n",
    "plot_mels(wav_paths=[input_wav_path, \"./BASIC5000_2296.wav\"], config=p_config)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "\r\n",
    "def devide_voice_from_path(input_path, output_path, preprocess_config):\r\n",
    "    silence_thresh = preprocess_config[\"preprocessing\"][\"audio\"][\"silence_thresh\"]\r\n",
    "    min_silence_len = preprocess_config[\"preprocessing\"][\"audio\"][\"min_silence_len\"]\r\n",
    "    keep_silence = preprocess_config[\"preprocessing\"][\"audio\"][\"keep_silence\"]\r\n",
    "\r\n",
    "    audio = AudioSegment.from_wav(input_path)\r\n",
    "\r\n",
    "    chunks = split_on_silence(audio, min_silence_len=min_silence_len,\r\n",
    "                              silence_thresh=silence_thresh,\r\n",
    "                              keep_silence=keep_silence)\r\n",
    "    for i, chunk in enumerate(chunks):\r\n",
    "        chunk.export(opth.join(output_path, opth.basename(input_path).replace(\".wav\", f\"_{i}.wav\")), format=\"wav\")\r\n",
    "\r\n",
    "    return len(chunks)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "\"\"\"\r\n",
    "Ricker = It is usually only referred to as the Mexican hat wavelet in the Americas\r\n",
    "\r\n",
    "fastspeech2: where ψ is the Mexican hat mother wavelet (Ryan, 1994), \r\n",
    "\r\n",
    "波形によって次元異なる問題. fastspeech2では, 普通に変換後同士でlossをとっているみたいだが,\r\n",
    "どうやって次元数をpredictとtargetでそろえたのか. iCWTをつかってpitch同士ならできると思うけどtrain時にそれはやっていないらしいので.\r\n",
    "\r\n",
    "\"\"\"\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "from glob import glob\r\n",
    "from wavelets_pytorch.transform import WaveletTransformTorch   # PyTorch version\r\n",
    "from wavelets_pytorch.wavelets import Ricker   # PyTorch version\r\n",
    "\r\n",
    "from utils.tools import pad_1D\r\n",
    "pitch = np.load(\"./preprocessed_data/N2C_4/source/pitch/pitch-n_c_beyond_tokkun-01_0.npy\")\r\n",
    "dt = 0.1         # sampling frequency\r\n",
    "dj = 0.125       # scale distribution parameter\r\n",
    "batch_size = 32  # how many signals to process in parallel\r\n",
    "\r\n",
    "# Batch of signals to process\r\n",
    "batch = []\r\n",
    "for pitch_path in glob(\"./preprocessed_data/N2C_4/source/pitch/*.npy\"):\r\n",
    "    batch.append(np.load(pitch_path))\r\n",
    "\r\n",
    "batch = pad_1D(batch[3:5])\r\n",
    "\r\n",
    "# Initialize wavelet filter banks (scipy and torch implementation)\r\n",
    "wa_torch = WaveletTransformTorch(dt, dj, cuda=False, wavelet=Ricker())\r\n",
    "\r\n",
    "# Performing wavelet transform (and compute scalogram)\r\n",
    "cwt_torch = wa_torch.cwt(batch)\r\n",
    "\r\n",
    "print(batch.shape)\r\n",
    "print(cwt_torch.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2, 224)\n",
      "(2, 71, 224)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "import matplotlib.pyplot as plt \r\n",
    "\r\n",
    "plt.close()\r\n",
    "plt.plot(batch[1])\r\n",
    "plt.savefig('figure01.jpg')\r\n",
    "plt.close()\r\n",
    "\r\n",
    "plt.plot(cwt_torch[1].T)\r\n",
    "plt.savefig('figure02.jpg')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e213ca6e6f25767a93bc05acc812b82627c61959a83a5ba54085851e25359099"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('NARS2S': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}